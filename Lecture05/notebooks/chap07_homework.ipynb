{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"chap07_homework.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python395jvsc74a57bd0815b746931c540dd900bc689bff56968bb61769b1159081e25b8d44643e03281","display_name":"Python 3.9.5 64-bit ('Python39')"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MUWcdth_khfN"},"source":["# 第5回講義 宿題"]},{"cell_type":"markdown","metadata":{"id":"gAjuP7I4lWyn"},"source":["## 課題\n","\n","今Lessonで学んだことに工夫を加えて、CNNでより高精度なCIFAR10の分類器を実装してみましょう。精度上位者はリーダーボードに載ります。"]},{"cell_type":"markdown","metadata":{"id":"Cpiz19GRlZ_9"},"source":["### 目標値\n","\n","Accuracy 78%"]},{"cell_type":"markdown","metadata":{"id":"qSHeI_utleEN"},"source":["### ルール\n","\n","- 訓練データはx_train、 t_train、テストデータはx_testで与えられます。\n","- 予測ラベルは one_hot表現ではなく0~9のクラスラベル で表してください。\n","- **下のセルで指定されているx_train、t_train以外の学習データは使わないでください。**\n","- 今回から基本的にAPI制限はありません。\n","- ただしtorchvision等の既存モデルや、学習済みモデルは用いないでください。"]},{"cell_type":"markdown","metadata":{"id":"diuec-_YluI6"},"source":["### 提出方法\n","\n","- 2つのファイルを提出していただきます。\n","  - テストデータ (x_test) に対する予測ラベルをcsvファイル (ファイル名: submission_pred.csv) で提出してください。\n","  - それに対応するpythonのコードをsubmission_code.pyとして提出してください (%%writefileコマンドなどを利用してください)。"]},{"cell_type":"markdown","metadata":{"id":"hofSzJsVlvKp"},"source":["### 評価方法\n","\n","- 予測ラベルのt_testに対する精度 (Accuracy) で評価します。\n","- 定時に採点しLeader Boardを更新します。(採点スケジュールは別アナウンス）\n","- 締切後の点数を最終的な評価とします。"]},{"cell_type":"markdown","metadata":{"id":"Cu4cmQtelx19"},"source":["### データの読み込み\n","\n","- この部分は修正しないでください"]},{"cell_type":"code","metadata":{"id":"EsLDDSUJkRx-"},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","from torchvision import transforms\n","from tqdm import tqdm_notebook as tqdm\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","\n","\"\"\"\n","#学習データ\n","x_train = np.load('drive/My Drive/Colab Notebooks/DLBasics2021_colab/Lecture_20210513/data/x_train.npy')\n","t_train = np.load('drive/My Drive/Colab Notebooks/DLBasics2021_colab/Lecture_20210513/data/t_train.npy')\n","    \n","#テストデータ\n","x_test = np.load('drive/My Drive/Colab Notebooks/DLBasics2021_colab/Lecture_20210513/data/x_test.npy')\n","\"\"\"\n","#学習データ\n","x_train = np.load('/Users/明朗/workspace/deeplearningUT/Lecture05/data/x_train.npy')\n","t_train = np.load('/Users/明朗/workspace/deeplearningUT/Lecture05/data/t_train.npy')\n","    \n","#テストデータ\n","x_test = np.load('/Users/明朗/workspace/deeplearningUT/Lecture05/data/x_test.npy')\n","\n","class train_dataset(torch.utils.data.Dataset):\n","    def __init__(self, x_train, t_train):\n","        data = x_train.astype('float32')\n","        self.x_train = []\n","        for i in range(data.shape[0]):\n","            self.x_train.append(Image.fromarray(np.uint8(data[i])))\n","        self.t_train = t_train\n","        self.transform = transforms.ToTensor()\n","\n","    def __len__(self):\n","        return len(self.x_train)\n","\n","    def __getitem__(self, idx):\n","        return self.transform(self.x_train[idx]), torch.tensor(t_train[idx], dtype=torch.long)\n","\n","class test_dataset(torch.utils.data.Dataset):\n","    def __init__(self, x_test):\n","        data = x_test.astype('float32')\n","        self.x_test = []\n","        for i in range(data.shape[0]):\n","            self.x_test.append(Image.fromarray(np.uint8(data[i])))\n","        self.transform = transforms.ToTensor()\n","\n","    def __len__(self):\n","        return len(self.x_test)\n","\n","    def __getitem__(self, idx):\n","        return self.transform(self.x_test[idx])\n","\n","trainval_data = train_dataset(x_train, t_train)\n","test_data = test_dataset(x_test)\n","\n","print(len(trainval_data))"],"execution_count":39,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'train_dataset' object has no attribute 'astype'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-39-be6ac897716b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mtrainval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-39-be6ac897716b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x_train, t_train)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mAttributeError\u001b[0m: 'train_dataset' object has no attribute 'astype'"]}]},{"cell_type":"markdown","metadata":{"id":"UrSpHDIWOfK_"},"source":["### 畳み込みニューラルネットワーク(CNN)の実装"]},{"cell_type":"code","metadata":{"id":"sKAe0F36nSvU"},"source":["class gcn():\n","    def __init__(self):\n","        pass\n","\n","    def __call__(self, x):\n","        mean = torch.mean(x)\n","        std = torch.std(x)\n","        return (x - mean)/(std + 10**(-6))  # 0除算を防ぐ\n","\n","\n","class ZCAWhitening():\n","    def __init__(self, epsilon=1e-4, device=\"cuda\"):  # 計算が重いのでGPUを用いる\n","        self.epsilon = epsilon\n","        self.device = device\n","\n","    def fit(self, images):  # 変換行列と平均をデータから計算\n","        x = images[0][0].reshape(1, -1)\n","        self.mean = torch.zeros([1, x.size()[1]]).to(self.device)\n","        con_matrix = torch.zeros([x.size()[1], x.size()[1]]).to(self.device)\n","        for i in range(len(images)):  # 各データについての平均を取る\n","            x = images[i][0].reshape(1, -1).to(self.device)\n","            self.mean += x / len(images)\n","            con_matrix += torch.mm(x.t(), x) / len(images)\n","            if i % 10000 == 0:\n","                print(\"{0}/{1}\".format(i, len(images)))\n","        self.E, self.V = torch.symeig(con_matrix, eigenvectors=True)  # 固有値分解\n","        self.E = torch.max(self.E, torch.zeros_like(self.E)) # 誤差の影響で負になるのを防ぐ\n","        self.ZCA_matrix = torch.mm(torch.mm(self.V, torch.diag((self.E.squeeze()+self.epsilon)**(-0.5))), self.V.t())\n","        print(\"completed!\")\n","\n","    def __call__(self, x):\n","        size = x.size()\n","        x = x.reshape(1, -1).to(self.device)\n","        x -= self.mean\n","        x = torch.mm(x, self.ZCA_matrix.t())\n","        x = x.reshape(tuple(size))\n","        x = x.to(\"cpu\")\n","        return x\n","\n","\n","# (datasetのクラスを自作したので、このあたりの処理が少し変わっています)\n","\n","zca = ZCAWhitening()\n","zca.fit(trainval_data)\n","\n","val_size = 3000\n","train_data, val_data = torch.utils.data.random_split(trainval_data, [len(trainval_data)-val_size, val_size])\n","\n","\n","# 前処理を定義\n","transform_train = transforms.Compose([transforms.RandomCrop(32, padding=(4, 4, 4, 4), padding_mode='constant'),\n","                                transforms.RandomHorizontalFlip(p=0.5),\n","                                transforms.RandomVerticalFlip(p=0.5),\n","                                transforms.RandomRotation(degrees=15),\n","                                transforms.ToTensor(),\n","                                zca])\n","\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                zca])\n","\n","trainval_data.transform = transform_train\n","test_data.transform = transform\n","\n","batch_size = 64\n","\n","dataloader_train = torch.utils.data.DataLoader(\n","    train_data,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","dataloader_valid = torch.utils.data.DataLoader(\n","    val_data,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","dataloader_test = torch.utils.data.DataLoader(\n","    test_data,\n","    batch_size=batch_size,\n","    shuffle=False\n",")\n"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["0/50000\n","10000/50000\n","20000/50000\n","30000/50000\n","40000/50000\n","completed!\n"]}]},{"cell_type":"code","metadata":{"id":"PADQiKNa2snb"},"source":["import torch.nn as nn\n","import torch.optim as optim\n","import torch.autograd as autograd\n","import torch.nn.functional as F\n","\n","rng = np.random.RandomState(1234)\n","random_state = 42\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","conv_net = nn.Sequential(\n","    nn.Conv2d(3, 64, 3, padding=1),              # 32x32x3 -> 32x32x64\n","    nn.BatchNorm2d(64),\n","    nn.ReLU(),\n","    nn.Conv2d(64, 64, 3, padding=1),            \n","    nn.BatchNorm2d(64),\n","    nn.ReLU(),\n","\n","    nn.MaxPool2d(2),                  # 32x32x64 -> 16x16x64\n","\n","    nn.Conv2d(64, 128, 3, padding=1),            # 16x16x64 -> 16x16x128\n","    nn.BatchNorm2d(128),\n","    nn.ReLU(),\n","    nn.Conv2d(128, 128, 3, padding=1),           \n","    nn.BatchNorm2d(128),\n","    nn.ReLU(),\n","\n","    nn.MaxPool2d(2),                  # 16x16x128 -> 8x8x128\n","\n","    nn.Conv2d(128, 256, 3, padding=1),            # 8x8x128 -> 8x8x256\n","    nn.BatchNorm2d(256),\n","    nn.ReLU(),\n","    nn.Conv2d(256, 256, 3, padding=1),            \n","    nn.BatchNorm2d(256),\n","    nn.ReLU(),\n","    nn.Conv2d(256, 256, 3, padding=1),            \n","    nn.BatchNorm2d(256),\n","    nn.ReLU(),\n","\n","    nn.MaxPool2d(2),                  # 8x8x256 -> 4x4x256\n","\n","    nn.Conv2d(256, 512, 3, padding=1),            # 4x4x256 -> 4x4x512\n","    nn.BatchNorm2d(512),\n","    nn.ReLU(),\n","    nn.Conv2d(512, 512, 3, padding=1),            \n","    nn.BatchNorm2d(512),\n","    nn.ReLU(),\n","    nn.Conv2d(512, 512, 3, padding=1),            \n","    nn.BatchNorm2d(512),\n","    nn.ReLU(),\n","\n","    nn.MaxPool2d(2),                  # 4x4x512 -> 2x2x512\n","    \n","    nn.Flatten(),\n","    nn.Linear(2*2*512, 256),\n","    nn.ReLU(),\n","    nn.Linear(256, 10)\n",")# WRITE ME\n","\n","\n","def init_weights(m):  # Heの初期化\n","    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n","        torch.nn.init.kaiming_normal_(m.weight)\n","        m.bias.data.fill_(0.0)\n","\n","\n","conv_net.apply(init_weights)\n","\n","\n","n_epochs = 100\n","lr = 0.01\n","device = 'cuda'\n","\n","conv_net.to(device)\n","optimizer = optim.Adam(conv_net.parameters(), lr=lr) # WRITE ME\n","loss_function = nn.CrossEntropyLoss() # WRITE ME"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlOZuLu-328i"},"source":["from tqdm import tqdm\n","pre_loss = 2\n","break_point = 0\n","\n","for epoch in tqdm(range(n_epochs)):\n","    losses_train = []\n","    losses_valid = []\n","\n","    conv_net.train()\n","    n_train = 0\n","    acc_train = 0\n","    for x, t in dataloader_train:\n","        # WRITE ME\n","        n_train += t.size()[0]\n","        conv_net.zero_grad()\n","        x = x.to(device)\n","        t = t.to(device)\n","        y = conv_net.forward(x)\n","        loss = loss_function(y,t)\n","        loss.backward()\n","        optimizer.step()\n","        pred = y.argmax(1)\n","\n","        acc_train += (pred == t).float().sum().item()\n","        losses_train.append(loss.tolist())\n","\n","    conv_net.eval()\n","    n_val = 0\n","    acc_val = 0\n","    for x, t in dataloader_valid:\n","        # WRITE ME\n","        n_val += t.size()[0]\n","        conv_net.zero_grad()\n","        x = x.to(device)\n","        t = t.to(device)\n","        y = conv_net.forward(x)\n","        loss = loss_function(y,t)\n","        pred = y.argmax(1)\n","\n","\n","        acc_val += (pred == t).float().sum().item()\n","        losses_valid.append(loss.tolist())\n","\n","    mean_lossval = np.mean(losses_valid)\n","    if (epoch+1)%5 == 0:\n","        print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]'.format(\n","            epoch,\n","            np.mean(losses_train),\n","            acc_train/n_train,\n","            mean_lossval,\n","            acc_val/n_val\n","        ))\n","\n","    if pre_loss < mean_lossval:\n","        break_point += 1\n","    else:\n","        pre_loss = mean_lossval\n","        break_point = 0\n","\n","    if break_point == 10:\n","        print(\"break\")\n","        print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]'.format(\n","            epoch,\n","            np.mean(losses_train),\n","            acc_train/n_train,\n","            mean_lossval,\n","            acc_val/n_val\n","        ))\n","\n","        break\n","    \n"],"execution_count":52,"outputs":[{"output_type":"stream","name":"stderr","text":["  5%|▌         | 5/100 [04:10<1:19:25, 50.17s/it]EPOCH: 4, Train [Loss: 0.881, Accuracy: 0.690], Valid [Loss: 0.927, Accuracy: 0.686]\n"," 10%|█         | 10/100 [08:21<1:15:07, 50.09s/it]EPOCH: 9, Train [Loss: 0.753, Accuracy: 0.740], Valid [Loss: 0.803, Accuracy: 0.721]\n"," 15%|█▌        | 15/100 [12:30<1:10:47, 49.97s/it]EPOCH: 14, Train [Loss: 0.666, Accuracy: 0.771], Valid [Loss: 0.719, Accuracy: 0.758]\n"," 20%|██        | 20/100 [16:41<1:06:40, 50.01s/it]EPOCH: 19, Train [Loss: 0.596, Accuracy: 0.796], Valid [Loss: 0.665, Accuracy: 0.766]\n"," 25%|██▌       | 25/100 [20:52<1:02:50, 50.27s/it]EPOCH: 24, Train [Loss: 0.555, Accuracy: 0.810], Valid [Loss: 0.629, Accuracy: 0.789]\n"," 30%|███       | 30/100 [25:04<58:47, 50.40s/it]EPOCH: 29, Train [Loss: 0.517, Accuracy: 0.823], Valid [Loss: 0.614, Accuracy: 0.793]\n"," 35%|███▌      | 35/100 [29:16<54:38, 50.43s/it]EPOCH: 34, Train [Loss: 0.488, Accuracy: 0.833], Valid [Loss: 0.600, Accuracy: 0.794]\n"," 40%|████      | 40/100 [33:28<50:26, 50.44s/it]EPOCH: 39, Train [Loss: 0.466, Accuracy: 0.842], Valid [Loss: 0.585, Accuracy: 0.803]\n"," 45%|████▌     | 45/100 [37:40<46:03, 50.24s/it]EPOCH: 44, Train [Loss: 0.444, Accuracy: 0.847], Valid [Loss: 0.577, Accuracy: 0.810]\n"," 50%|█████     | 50/100 [41:49<41:39, 49.98s/it]EPOCH: 49, Train [Loss: 0.425, Accuracy: 0.855], Valid [Loss: 0.577, Accuracy: 0.822]\n"," 55%|█████▌    | 55/100 [46:00<37:35, 50.13s/it]EPOCH: 54, Train [Loss: 0.406, Accuracy: 0.861], Valid [Loss: 0.539, Accuracy: 0.821]\n"," 60%|██████    | 60/100 [50:11<33:25, 50.13s/it]EPOCH: 59, Train [Loss: 0.396, Accuracy: 0.864], Valid [Loss: 0.629, Accuracy: 0.796]\n"," 65%|██████▌   | 65/100 [54:21<29:10, 50.01s/it]EPOCH: 64, Train [Loss: 0.381, Accuracy: 0.869], Valid [Loss: 0.535, Accuracy: 0.820]\n"," 70%|███████   | 70/100 [58:31<25:02, 50.08s/it]EPOCH: 69, Train [Loss: 0.367, Accuracy: 0.874], Valid [Loss: 0.554, Accuracy: 0.813]\n"," 72%|███████▏  | 72/100 [1:01:01<23:44, 50.86s/it]break\n","EPOCH: 72, Train [Loss: 0.360, Accuracy: 0.876], Valid [Loss: 0.521, Accuracy: 0.826]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"Yq3scS5j4Rt2"},"source":["conv_net.eval()\n","\n","t_pred = []\n","for x in dataloader_test:\n","\n","    x = x.to(device)\n","\n","    # 順伝播\n","    y = conv_net.forward(x)\n","\n","    # モデルの出力を予測値のスカラーに変換\n","    pred = y.argmax(1).tolist()\n","\n","    t_pred.extend(pred)\n","\n","submission = pd.Series(t_pred, name='label')\n","\"\"\"\n","submission.to_csv('drive/My Drive/Colab Notebooks/DLBasics2021_colab/Lecture_20210513/submission_pred_sub.csv', header=True, index_label='id')\n","\"\"\"\n","import os\n","os.makedirs('output', exist_ok=True)\n","submission.to_csv('output/submission_pred_moredataAg_break_%d'%n_epochs+'.csv', header=True, index_label='id')"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"73f8DjqTCoTL"},"source":[],"execution_count":null,"outputs":[]}]}